{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert to a DataFrame\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "\n",
    "# Filter for binary classification 'versicolor' vs 'virginica' ['setosa' 'versicolor' 'virginica']\n",
    "X = iris_df[iris_df['species'].isin([1, 2])].copy()\n",
    "\n",
    "# Relabel the target column (optional: make species binary 0 and 1)\n",
    "X['species'] = X['species'].map({1: 0, 2: 1})\n",
    "\n",
    "#to array\n",
    "X = X.values\n",
    "\n",
    "# Print the filtered dataset\n",
    "X.shape # (100, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4)\n",
      "(20, 4)\n",
      "(80, 1)\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train = X_train[:, -1].reshape(-1, 1)\n",
    "y_test = X_test[:, -1].reshape(-1, 1)\n",
    "X_train = X_train[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "print(X_train.shape) # (80, 5) m examples, n features\n",
    "print(X_test.shape)  # (20, 5)\n",
    "print(y_train.shape) # (80,1)\n",
    "print(y_test.shape)  # (20,1)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:58: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_2764235/2533989960.py:58: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork1Layer:\n",
    "    def __init__(self, input_dim , output_dim, nunits=2, learning_rate=0.01, n_iters=1000):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.nunits = nunits\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.parameters = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        '''Sigmoid function that works with numpy arrays'''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        '''Initialize weights and bias randomly\n",
    "        initializing to zero results in hidden units to be identical'''\n",
    "        W1 = np.random.randn(self.nunits, self.input_dim) * 0.01 #small random numbers\n",
    "        b1 = np.zeros((self.nunits, 1))\n",
    "        W2 = np.random.randn(self.output_dim, self.nunits) * 0.01\n",
    "        b2 = np.zeros((self.output_dim, 1))\n",
    "        self.parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        X -- input data of size (n_x, m)\n",
    "        \n",
    "        Returns:\n",
    "        A2 -- The sigmoid output of the second activation\n",
    "        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "\n",
    "        Forward:\n",
    "        z^1 = w^1 . X + b^1 ; A^1 = tanh(z^1)\n",
    "        Z^2 = W^2 . A^1 + b^2 ; A^2 = sigmoid(z^2)\n",
    "        \"\"\"      \n",
    "        W1 = self.parameters[\"W1\"]\n",
    "        b1 = self.parameters[\"b1\"]\n",
    "        W2 = self.parameters[\"W2\"]\n",
    "        b2 = self.parameters[\"b2\"]\n",
    "        #W1 is shape(nunits, input_dim), \n",
    "        #X is shape(input_dim, number of examples), each col is an example\n",
    "        #b1 is shape(nunits, 1)\n",
    "        #W2 is shape (1, nunits)\n",
    "\n",
    "        Z1 =  np.dot(W1, X) + b1 # (nunits,number of examples)\n",
    "        A1 = np.tanh(Z1) #(nunits,number of examples)\n",
    "        Z2 = np.dot(W2, A1) + b2 #(1,number of examples)\n",
    "        A2 = self.sigmoid(Z2)\n",
    "\n",
    "        cache = {\"Z1\": Z1,\n",
    "            \"A1\": A1,\n",
    "            \"Z2\": Z2,\n",
    "            \"A2\": A2}\n",
    "        return A2, cache\n",
    "\n",
    "    def compute_cost(self, A2, Y):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy cost given in equation:\n",
    "        $$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "        Arguments:\n",
    "        A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "        Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "        Returns:\n",
    "        cost -- cross-entropy cost given equation \n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y)\n",
    "        cost = -1/m * np.sum(logprobs)\n",
    "        return float(np.squeeze(cost))\n",
    "\n",
    "    def backward_propagation(self, cache, X, Y):    \n",
    "        \"\"\"\n",
    "            Implement the backward propagation-\n",
    "            Arguments:\n",
    "            cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "            X -- input data of shape (2, number of examples)\n",
    "            Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "            Returns:\n",
    "            grads -- python dictionary containing your gradients with respect to different parameters\n",
    "        \"\"\"\n",
    "        #backpropagation:\n",
    "        #dZ^2 = A^2 - Y\n",
    "        #dW^2 = 1/m dZ^2 . A^1^T\n",
    "        #db^2 = 1/m np.sum(dZ^2,axis=1, keepdims=True)\n",
    "        #dZ^1 = W^2^T dZ^2 * activation func' (Z^1)  \n",
    "        #dW^1 = 1/m dZ^1 . X^T\n",
    "        #db^1 = 1/m np.sum(dZ^1, axis=1, keepdims=True)'''\n",
    "        m = X.shape[1]\n",
    "        W1 = self.parameters[\"W1\"]\n",
    "        W2 = self.parameters[\"W2\"]\n",
    "        A1 = cache[\"A1\"]\n",
    "        A2 = cache[\"A2\"]\n",
    "\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "        db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.dot(W2.T, dZ2) * (1-np.power(A1,2)) # with tanh\n",
    "        dW1 = 1/m * np.dot(dZ1,X.T)\n",
    "        db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "\n",
    "        W1 = self.parameters[\"W1\"]\n",
    "        b1 = self.parameters[\"b1\"]\n",
    "        W2 = self.parameters[\"W2\"]\n",
    "        b2 = self.parameters[\"b2\"]\n",
    "\n",
    "        W1 -= self.lr * grads[\"dW1\"]\n",
    "        b1 -= self.lr * grads[\"db1\"]\n",
    "        W2 -= self.lr * grads[\"dW2\"]\n",
    "        b2 -= self.lr * grads[\"db2\"]\n",
    "\n",
    "        return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        '''Fit according to the learning rate and number of iterations'''\n",
    "        np.random.seed(0)\n",
    "        m = X.shape[1]\n",
    "        input_dim = X.shape[0]\n",
    "        output_dim = 1 \n",
    "        costs = []\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            cost = self.compute_cost(A2, Y)\n",
    "            grads = self.backward_propagation(cache, X, Y)\n",
    "            self.parameters = self.update_parameters(grads)\n",
    "\n",
    "            # Print the cost every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "                print(f'Cost after iteration {i}: {cost}')\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Predict the class labels for the provided data'''\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        return (A2 > 0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6929754046453289\n",
      "Cost after iteration 100: 0.6920048920721933\n",
      "Cost after iteration 200: 0.6909831480720352\n",
      "Cost after iteration 300: 0.6898554364288609\n",
      "Cost after iteration 400: 0.6886120403917441\n",
      "Cost after iteration 500: 0.687140391902185\n",
      "Cost after iteration 600: 0.6852607918003812\n",
      "Cost after iteration 700: 0.6827266133098835\n",
      "Cost after iteration 800: 0.6791884275487005\n",
      "Cost after iteration 900: 0.6741427574374399\n",
      "Cost after iteration 1000: 0.6668812175433363\n",
      "Cost after iteration 1100: 0.6564642647124979\n",
      "Cost after iteration 1200: 0.6417450769824765\n",
      "Cost after iteration 1300: 0.621499081569636\n",
      "Cost after iteration 1400: 0.5947206695000163\n",
      "Cost after iteration 1500: 0.561072516187994\n",
      "Cost after iteration 1600: 0.52131025277525\n",
      "Cost after iteration 1700: 0.47737656546001733\n",
      "Cost after iteration 1800: 0.43197530704658665\n",
      "Cost after iteration 1900: 0.38780693579155073\n",
      "Cost after iteration 2000: 0.3469071430578405\n",
      "Cost after iteration 2100: 0.31039101240307004\n",
      "Cost after iteration 2200: 0.27856947165306\n",
      "Cost after iteration 2300: 0.2512264456764039\n",
      "Cost after iteration 2400: 0.22788212176743647\n",
      "Cost after iteration 2500: 0.20797358360197343\n",
      "Cost after iteration 2600: 0.19095461653996146\n",
      "Cost after iteration 2700: 0.17633993862853223\n",
      "Cost after iteration 2800: 0.16371791491325657\n",
      "Cost after iteration 2900: 0.1527479591417633\n",
      "Cost after iteration 3000: 0.14315182331652596\n",
      "Cost after iteration 3100: 0.1347034055125211\n",
      "Cost after iteration 3200: 0.127219134793475\n",
      "Cost after iteration 3300: 0.12054967925715601\n",
      "Cost after iteration 3400: 0.1145731096711349\n",
      "Cost after iteration 3500: 0.10918939632730523\n",
      "Cost after iteration 3600: 0.10431603410170616\n",
      "Cost after iteration 3700: 0.09988458449880153\n",
      "Cost after iteration 3800: 0.09583794701032276\n",
      "Cost after iteration 3900: 0.09212820390696823\n",
      "Cost after iteration 4000: 0.08871491346198368\n",
      "Cost after iteration 4100: 0.08556375327785751\n",
      "Cost after iteration 4200: 0.08264543716068343\n",
      "Cost after iteration 4300: 0.07993484623075574\n",
      "Cost after iteration 4400: 0.07741032838709172\n",
      "Cost after iteration 4500: 0.07505313060707593\n",
      "Cost after iteration 4600: 0.07284693652536338\n",
      "Cost after iteration 4700: 0.0707774878466018\n",
      "Cost after iteration 4800: 0.06883227283867831\n",
      "Cost after iteration 4900: 0.06700026876364376\n",
      "Cost after iteration 5000: 0.06527172788982126\n",
      "Cost after iteration 5100: 0.06363799888665211\n",
      "Cost after iteration 5200: 0.062091377081915713\n",
      "Cost after iteration 5300: 0.060624978371358454\n",
      "Cost after iteration 5400: 0.05923263259853764\n",
      "Cost after iteration 5500: 0.05790879303238769\n",
      "Cost after iteration 5600: 0.05664845921078206\n",
      "Cost after iteration 5700: 0.05544711092769938\n",
      "Cost after iteration 5800: 0.054300651548257306\n",
      "Cost after iteration 5900: 0.05320535916195812\n",
      "Cost after iteration 6000: 0.052157844347079166\n",
      "Cost after iteration 6100: 0.05115501353147307\n",
      "Cost after iteration 6200: 0.05019403710744519\n",
      "Cost after iteration 6300: 0.04927232159889619\n",
      "Cost after iteration 6400: 0.048387485293908565\n",
      "Cost after iteration 6500: 0.047537336850393644\n",
      "Cost after iteration 6600: 0.04671985646026605\n",
      "Cost after iteration 6700: 0.04593317922200797\n",
      "Cost after iteration 6800: 0.04517558042494344\n",
      "Cost after iteration 6900: 0.044445462493055324\n",
      "Cost after iteration 7000: 0.04374134337337987\n",
      "Cost after iteration 7100: 0.043061846185183855\n",
      "Cost after iteration 7200: 0.042405689972343646\n",
      "Cost after iteration 7300: 0.041771681423444756\n",
      "Cost after iteration 7400: 0.04115870744281122\n",
      "Cost after iteration 7500: 0.04056572847152367\n",
      "Cost after iteration 7600: 0.039991772470964804\n",
      "Cost after iteration 7700: 0.03943592949292123\n",
      "Cost after iteration 7800: 0.03889734677009669\n",
      "Cost after iteration 7900: 0.03837522426931086\n",
      "Cost after iteration 8000: 0.03786881065689068\n",
      "Cost after iteration 8100: 0.03737739963199278\n",
      "Cost after iteration 8200: 0.036900326588971286\n",
      "Cost after iteration 8300: 0.036436965574559346\n",
      "Cost after iteration 8400: 0.035986726509666694\n",
      "Cost after iteration 8500: 0.035549052649103574\n",
      "Cost after iteration 8600: 0.03512341825559123\n",
      "Cost after iteration 8700: 0.03470932646708829\n",
      "Cost after iteration 8800: 0.03430630733878957\n",
      "Cost after iteration 8900: 0.033913916043196955\n",
      "Cost after iteration 9000: 0.033531731213454444\n",
      "Cost after iteration 9100: 0.03315935341671491\n",
      "Cost after iteration 9200: 0.03279640374569689\n",
      "Cost after iteration 9300: 0.03244252251781498\n",
      "Cost after iteration 9400: 0.03209736807235245\n",
      "Cost after iteration 9500: 0.03176061565710391\n",
      "Cost after iteration 9600: 0.03143195639677104\n",
      "Cost after iteration 9700: 0.03111109633614847\n",
      "Cost after iteration 9800: 0.030797755551814555\n",
      "Cost after iteration 9900: 0.030491667326640215\n",
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = NeuralNetwork1Layer(input_dim=X_train.shape[0], output_dim=1, nunits=4, learning_rate=0.01, n_iters=10000)\n",
    "parameters = model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
